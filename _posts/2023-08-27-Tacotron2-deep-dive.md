---
layout: post
title:  "Tacotron 2: Elevating Text-to-Speech to New Horizons"
author: Sam3oul
categories: [ Artficial inteligence,Google,Model Architecture,Machine learning,deep learning]
image: assets/images/Tacotron2.png
beforetoc: "In this article we are going to explore a model proposed by google as a Text to speech technology which Tacotron 2 "
toc: true
---
## What's Tacotron 2 ?
Tacotron 2 is an **end-to-end** groundbreaking **neural network-based** architecture of `Text-to-Speech (TTS)` technology. Developed by `Google`'s AI research team, this model represents a significant advancement in synthesizing natural-sounding speech from text inputs. Tacotron 2 employs a two-stage approach, where the first stage generates a **mel-spectrogram**, a representation of the speech's frequency content, from the input text .

The second stage then converts this mel-spectrogram into the final **waveform** of `human-like` speech.

One of the standout features of Tacotron 2 is its ability to capture intricate `intonations`, `expressiveness`, and `contextual cues`, producing speech that sounds remarkably **authentic**.

We are set to uncover the core architecture of the model. 

For an in-depth exploration of both the model and its `underlying theory`, you have the option to refer to the official <a href='https://arxiv.org/pdf/1712.05884v2.pdf'> paper authored by **Google's research team**</a>.

## Architecture of Tacotron 2
The `Tacotron 2` system adopts an **encoder-decoder** architecture with the attention mechanism, which enables it to transform text inputs into natural-sounding speech. 

Here's a more detailed breakdown of how the architecture works:

![walking]({{ site.baseurl }}/assets/images/TacotronArchi.png)


**1-Encoder**

The Encoder is responsible for taking a sequence of characters (letters) as input, which represents the textual information that needs to be converted into speech. The character sequence is first processed by character embeddings, which are vector representations that capture the characteristics of each character. These embeddings are then passed through a stack of convolutional layers. This stack of convolutional layers helps in extracting higher-level features from the character embeddings and creates a hidden representation that captures the phonetic and linguistic features of the input text. This hidden representation is essentially a condensed representation of the input sequence's content.
Attention Mechanism

**2-The attention mechanism**

it is a crucial component of the Decoder, which generates the mel-spectrogram representation of the speech. The attention mechanism works in tandem with the hidden representation generated by the Encoder. The idea behind attention is to dynamically focus on different parts of the hidden representation as the Decoder generates each step of the output.
Decoder

**3-The Decoder**

It takes the hidden representation produced by the `Encoder` and generates the `mel-spectrogram` step by step. The attention mechanism guides the Decoder's focus by assigning different weights to different parts of the hidden representation at each step.

This dynamic attention helps the model to align the generated mel-spectrogram with the relevant parts of the input text, capturing the **prosody** and **intonation** accurately.

## Testing the model

open google colab and start by installing the TTS pachage :

```python
!pip install tts
```

Then let's import the Torch and Librosa libraries and the IPython.display module.

```python
import torch
import librosa
import IPython.display as ipd
```

you could run this command to view all the pre-trained models provided by **Coqui-ai TTS**

```python
!tts --list_models
```

to test a model you simply have to run this command

```python
!tts --text "<Put your sentence here>" \
    --model_name "<Model name goes here>" \
    --out_path output.wav
```

for example to test the **Tacotron 2** model ,you simply have to run this command :

```python
!tts --text "hi ,I wish you are enjoying this article" \
    --model_name "tts_models/en/ek1/tacotron2" \
    --out_path output.wav
```

Finally to open the `output audio` ,simply run this command :

```python
x, sr = librosa.load('output.wav')
ipd.Audio(x, rate=sr)
```
You could visit this<a href ='https://soundcloud.com/ismail-said-988997578/tacotron-2-result?si=5317450d6b204e128bd4ae499f9194bf&utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing'>link</a> 
to check the result

Not bad, isn't it? 
to be honest the result was `pretty good` but you may notice that the sound is `robotic` and lacks `emotions`. 

That's why we will use another tool called Elevenlabs in the next Article .
